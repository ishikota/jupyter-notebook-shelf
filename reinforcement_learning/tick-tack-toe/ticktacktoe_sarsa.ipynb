{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tick-tack-toe agent by \"Sarsa\"\n",
    "## Reference\n",
    "\n",
    "Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)\n",
    "  - chapter 6.4 Sarsa: On-Policy TD Control\n",
    "  - https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node64.html\n",
    "\n",
    "## algorithm\n",
    "<img src=\"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp8.png\" />\n",
    "https://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp8.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions to play tick-tack-toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_board(state):\n",
    "    board = [-1 for i in range(9)]\n",
    "    for i in range(2):\n",
    "        for j in range(9):\n",
    "            if state[i] >> j &1 == 1:\n",
    "                board[j] = i\n",
    "    icon_map = {-1: \"-\", 0:\"o\", 1:\"x\"}\n",
    "    board = map(lambda i: icon_map[i], board)\n",
    "    for i in range(3):\n",
    "        print \"%s %s %s\" % tuple(board[i*3:(i+1)*3])\n",
    "\n",
    "# return empty cell position on the board\n",
    "def possible_actions(state):\n",
    "    board = state[0] | state[1]\n",
    "    def add(ary, pos):\n",
    "        if (board >> pos)&1 == 0:\n",
    "            ary.append(1<<pos)\n",
    "        return ary\n",
    "    return reduce(add, range(9), [])\n",
    "\n",
    "def is_terminated(player_board):\n",
    "    bin2i = lambda b: int(b, 2)\n",
    "    line_horizon = any([player_board & mask == mask for mask in map(bin2i, ['000000111', '000111000', '111000000'])])\n",
    "    line_vertical = any([player_board & mask == mask for mask in map(bin2i, ['001001001', '010010010', '100100100'])])\n",
    "    line_diagonal = any([player_board & mask == mask for mask in map(bin2i, ['100010001', '001010100'])])\n",
    "    return line_horizon | line_vertical | line_diagonal\n",
    "\n",
    "def is_draw(state):\n",
    "    return len(possible_actions(state))==0\n",
    "\n",
    "def calc_reward(state):\n",
    "    first_player_board, second_player_board = state\n",
    "    if (is_terminated(first_player_board)):\n",
    "        return 1\n",
    "    elif(is_terminated(second_player_board)):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def apply_action(state, action):\n",
    "    first_player_board ,second_player_board = state\n",
    "    count_move = lambda : bin(first_player_board | second_player_board).count(\"1\")\n",
    "    if count_move()%2==0:\n",
    "        first_player_board |= action\n",
    "    else:\n",
    "        second_player_board |= action\n",
    "    return first_player_board, second_player_board\n",
    "\n",
    "def play_a_game():\n",
    "    state = (0,0)\n",
    "    visualize_board(state)\n",
    "    while not any(map(is_terminated, state) + [is_draw(state)]):\n",
    "        act = int(raw_input(\"action => %s\" % possible_actions(state)))\n",
    "        state = apply_action(state, act)\n",
    "        visualize_board(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions for GPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Play 1 game and return Q-value (state, reward pair) array watched in the game\n",
    "def gen_episode(Q, debug=False):\n",
    "    state = (0, 0)\n",
    "    episode = []\n",
    "    while not any(map(is_terminated, state) + [is_draw(state)]):\n",
    "        action = policy(Q, state, debug=debug)\n",
    "        episode.append((state, action))\n",
    "        _, state = transition(state, action)\n",
    "        if debug: visualize_board(state)\n",
    "    return episode, calc_reward(state)\n",
    "\n",
    "import math, random\n",
    "# Epsiron-greedy\n",
    "def policy(Q, current_state, eps=0.1, debug=False):\n",
    "    do_random = lambda : random.random() < eps\n",
    "    transition_curry = lambda action: transition(current_state, action)\n",
    "    Q_value_curry = lambda action: Q[current_state[0]][current_state[1]][int(math.log(action,2))]\n",
    "    if do_random():\n",
    "        if debug: print \"do random\"\n",
    "        return random.choice(possible_actions(current_state))\n",
    "    else:\n",
    "        if debug: print \"do greedy\"\n",
    "        Q_value_for_actions = map(Q_value_curry, possible_actions(current_state))\n",
    "        best_act_idx = Q_value_for_actions.index(max(Q_value_for_actions))\n",
    "        return possible_actions(current_state)[best_act_idx]\n",
    "\n",
    "# return  next state and reward after passed action is applied\n",
    "def transition(state, action):\n",
    "    next_state =  apply_action(state, action)\n",
    "    reward = calc_reward(next_state)\n",
    "    return reward, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# policy evaluation => policy improvement process\n",
    "def Sarsa(Q, alpha=0.1, gamma=0.1):\n",
    "    episode, reward = gen_episode(Q)\n",
    "    for state, action in episode:\n",
    "        next_reward, next_state = transition(state, action)\n",
    "        is_terminal_state = any(map(is_terminated, next_state) + [is_draw(next_state)])\n",
    "        next_action = policy(Q, next_state) if not is_terminal_state else 1   # How to fetch next_action from terminal state??\n",
    "        s1, s2, a = state[0], state[1], int(math.log(action,2))\n",
    "        ns1, ns2, na = next_state[0], next_state[1], int(math.log(next_action, 2))\n",
    "        Q[s1][s2][a]= Q[s1][s2][a] + alpha * ( next_reward + gamma * Q[ns1][ns2][na] -  Q[s1][s2][a])\n",
    "\n",
    "def display_Q_value(Q, state):\n",
    "    Q_value_curry = lambda action: Q[state[0]][state[1]][int(math.log(action,2))]\n",
    "    Q_values =map(Q_value_curry, [1<<n for n in range(9)])\n",
    "    max_len = max([len(str(val)) for val in Q_values])\n",
    "    for i in range(3):\n",
    "        line = []\n",
    "        for j in range(3):\n",
    "            pos = 3*i+j\n",
    "            board = state[0] | state[1]\n",
    "            line.append(str(Q_values[pos]) if (board>>pos)&1 == 0 else \"x\")\n",
    "        print \"%s %s %s\" % tuple([elem.rjust(max_len, '0') for elem in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start learning !! Iterate GPI process for 50000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.19680241422e-05 6.57291627587e-05 7.94095934152e-05\n",
      "6.61682847706e-05 07.6603597521e-05 6.44298268849e-05\n",
      "7.90297811993e-05 6.74716961773e-05 6.97592852368e-05\n"
     ]
    }
   ],
   "source": [
    "Q_table = [[[0 for a in range(9)] for j in range(2**9)] for i in range(2**9)]\n",
    "\n",
    "# Improve agent by iterating GPI process\n",
    "for i in range(50000):\n",
    "    Sarsa(Q_table)\n",
    "display_Q_value(Q_table, (0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions to play with agent through console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_next_player_agent(state):\n",
    "    first_player_board ,second_player_board = state\n",
    "    count_move = lambda : bin(first_player_board | second_player_board).count(\"1\")\n",
    "    return count_move() % 2 == 0\n",
    "\n",
    "def play_with_agent():\n",
    "    state = (0,0)\n",
    "    while not any(map(is_terminated, state) + [is_draw(state)]):\n",
    "        action, player = None, None\n",
    "        if is_next_player_agent(state):\n",
    "            display_Q_value(Q_table, state)\n",
    "            action = policy(Q_table, state)\n",
    "            player = \"agent\"\n",
    "        else:\n",
    "            action = int(raw_input(\"action => %s\" % possible_actions(state)))\n",
    "            player = \"you\"\n",
    "        state = apply_action(state, action)\n",
    "        visualize_board(state)\n",
    "        print \"player : %s, action : %d\" % (player, int(math.log(action, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play tick-tack-toe with our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.19680241422e-05 6.57291627587e-05 7.94095934152e-05\n",
      "6.61682847706e-05 07.6603597521e-05 6.44298268849e-05\n",
      "7.90297811993e-05 6.74716961773e-05 6.97592852368e-05\n",
      "o - -\n",
      "- - -\n",
      "- - -\n",
      "player : agent, action : 0\n",
      "action => [2, 4, 8, 16, 32, 64, 128, 256]2\n",
      "o x -\n",
      "- - -\n",
      "- - -\n",
      "player : you, action : 1\n",
      "0000000000000000x 0000000000000000x 8.52016341286e-05\n",
      "00.00790387425769 00.00779503256187 7.91517850475e-05\n",
      "00.00891134215165 08.2244066792e-05 00.00709218319692\n",
      "o x -\n",
      "- - -\n",
      "o - -\n",
      "player : agent, action : 6\n",
      "action => [4, 8, 16, 32, 128, 256]16\n",
      "o x -\n",
      "- x -\n",
      "o - -\n",
      "player : you, action : 4\n",
      "000000000000000x 000000000000000x 0.00844502959246\n",
      "00000000000001.0 000000000000000x 0.00772545428489\n",
      "000000000000000x 00.0086562767804 0.00819604536569\n",
      "o x -\n",
      "o x -\n",
      "o - -\n",
      "player : agent, action : 3\n"
     ]
    }
   ],
   "source": [
    "play_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>> Good!! Agent seems understanding how to win!!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.19680241422e-05 6.57291627587e-05 7.94095934152e-05\n",
      "6.61682847706e-05 07.6603597521e-05 6.44298268849e-05\n",
      "7.90297811993e-05 6.74716961773e-05 6.97592852368e-05\n",
      "o - -\n",
      "- - -\n",
      "- - -\n",
      "player : agent, action : 0\n",
      "action => [2, 4, 8, 16, 32, 64, 128, 256]8\n",
      "o - -\n",
      "x - -\n",
      "- - -\n",
      "player : you, action : 3\n",
      "0000000000000000x 00.00861475209802 0.000323658975269\n",
      "0000000000000000x 0.000908143497353 01.2664670074e-07\n",
      "06.6676853621e-05 1.59608407136e-07 1.96507945238e-05\n",
      "o o -\n",
      "x - -\n",
      "- - -\n",
      "player : agent, action : 1\n",
      "action => [4, 16, 32, 64, 128, 256]4\n",
      "o o x\n",
      "x - -\n",
      "- - -\n",
      "player : you, action : 2\n",
      "000000000000000x 000000000000000x 000000000000000x\n",
      "000000000000000x 0.00945520817625 00000000000000.0\n",
      "00000000000000.0 00000000000000.0 00000000000000.0\n",
      "o o x\n",
      "x o -\n",
      "- - -\n",
      "player : agent, action : 4\n",
      "action => [32, 64, 128, 256]128\n",
      "o o x\n",
      "x o -\n",
      "- x -\n",
      "player : you, action : 7\n",
      "000000000000000x 000000000000000x 000000000000000x\n",
      "000000000000000x 000000000000000x 0.00178408605053\n",
      "0000000000000000 000000000000000x 0000000000000000\n",
      "o o x\n",
      "x o o\n",
      "- x -\n",
      "player : agent, action : 5\n",
      "action => [64, 256]256\n",
      "o o x\n",
      "x o o\n",
      "- x x\n",
      "player : you, action : 8\n",
      "00x 00x 00x\n",
      "00x 00x 00x\n",
      "0.0 00x 00x\n",
      "o o x\n",
      "x o o\n",
      "o x x\n",
      "player : agent, action : 6\n"
     ]
    }
   ],
   "source": [
    "play_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>> Woops... Agent made a silly mistake :(`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>> It seems that 50000 GPI iteration was too small (Most of states are never experienced)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tick-tack-toe agent by \"MonteCalroES\"\n",
    "## Reference\n",
    "\n",
    "Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)\n",
    "  - chapter 5.3\n",
    "  - https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node53.html\n",
    "\n",
    "## algorithm\n",
    "<img src=\"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp4.png\" />\n",
    "https://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions to play tick-tack-toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_board(state):\n",
    "    board = [-1 for i in range(9)]\n",
    "    for i in range(2):\n",
    "        for j in range(9):\n",
    "            if state[i] >> j &1 == 1:\n",
    "                board[j] = i\n",
    "    icon_map = {-1: \"-\", 0:\"o\", 1:\"x\"}\n",
    "    board = map(lambda i: icon_map[i], board)\n",
    "    for i in range(3):\n",
    "        print \"%s %s %s\" % tuple(board[i*3:(i+1)*3])\n",
    "\n",
    "# return empty cell position on the board\n",
    "def possible_actions(state):\n",
    "    board = state[0] | state[1]\n",
    "    def add(ary, pos):\n",
    "        if (board >> pos)&1 == 0:\n",
    "            ary.append(1<<pos)\n",
    "        return ary\n",
    "    return reduce(add, range(9), [])\n",
    "\n",
    "def is_terminated(player_board):\n",
    "    bin2i = lambda b: int(b, 2)\n",
    "    line_horizon = any([player_board & mask == mask for mask in map(bin2i, ['000000111', '000111000', '111000000'])])\n",
    "    line_vertical = any([player_board & mask == mask for mask in map(bin2i, ['001001001', '010010010', '100100100'])])\n",
    "    line_diagonal = any([player_board & mask == mask for mask in map(bin2i, ['100010001', '001010100'])])\n",
    "    return line_horizon | line_vertical | line_diagonal\n",
    "\n",
    "def is_draw(state):\n",
    "    return len(possible_actions(state))==0\n",
    "\n",
    "def calc_reward(state):\n",
    "    first_player_board, second_player_board = state\n",
    "    if (is_terminated(first_player_board)):\n",
    "        return 1\n",
    "    elif(is_terminated(second_player_board)):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def apply_action(state, action):\n",
    "    first_player_board ,second_player_board = state\n",
    "    count_move = lambda : bin(first_player_board | second_player_board).count(\"1\")\n",
    "    if count_move()%2==0:\n",
    "        first_player_board |= action\n",
    "    else:\n",
    "        second_player_board |= action\n",
    "    return first_player_board, second_player_board\n",
    "\n",
    "def play_a_game():\n",
    "    state = (0,0)\n",
    "    visualize_board(state)\n",
    "    while not any(map(is_terminated, state) + [is_draw(state)]):\n",
    "        act = int(raw_input(\"action => %s\" % possible_actions(state)))\n",
    "        state = apply_action(state, act)\n",
    "        visualize_board(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - -\n",
      "- - -\n",
      "- - -\n",
      "action => [1, 2, 4, 8, 16, 32, 64, 128, 256]1\n",
      "o - -\n",
      "- - -\n",
      "- - -\n",
      "action => [2, 4, 8, 16, 32, 64, 128, 256]8\n",
      "o - -\n",
      "x - -\n",
      "- - -\n",
      "action => [2, 4, 16, 32, 64, 128, 256]32\n",
      "o - -\n",
      "x - o\n",
      "- - -\n",
      "action => [2, 4, 16, 64, 128, 256]16\n",
      "o - -\n",
      "x x o\n",
      "- - -\n",
      "action => [2, 4, 64, 128, 256]64\n",
      "o - -\n",
      "x x o\n",
      "o - -\n",
      "action => [2, 4, 128, 256]256\n",
      "o - -\n",
      "x x o\n",
      "o - x\n",
      "action => [2, 4, 128]128\n",
      "o - -\n",
      "x x o\n",
      "o o x\n",
      "action => [2, 4]4\n",
      "o - x\n",
      "x x o\n",
      "o o x\n",
      "action => [2]2\n",
      "o o x\n",
      "x x o\n",
      "o o x\n"
     ]
    }
   ],
   "source": [
    "play_a_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions for GPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Play 1 game and return Q-value (state, reward pair) array watched in the game\n",
    "def gen_episode(Q, debug=False):\n",
    "    state = (0, 0)\n",
    "    episode = []\n",
    "    while not any(map(is_terminated, state) + [is_draw(state)]):\n",
    "        action = policy(Q, state, eps=0.3, debug=debug)\n",
    "        episode.append((state, action))\n",
    "        _, state = transition(state, action)\n",
    "        if debug: visualize_board(state)\n",
    "    return episode, calc_reward(state)\n",
    "\n",
    "import math\n",
    "# Epsiron-greedy\n",
    "def policy(Q, current_state, eps=0.1, debug=False):\n",
    "    do_random = lambda : random.random() < eps\n",
    "    transition_curry = lambda action: transition(current_state, action)\n",
    "    Q_value_curry = lambda action: Q[current_state[0]][current_state[1]][int(math.log(action,2))]\n",
    "    if do_random():\n",
    "        if debug: print \"do random\"\n",
    "        return random.choice(possible_actions(current_state))\n",
    "    else:\n",
    "        if debug: print \"do greedy\"\n",
    "        Q_value_for_actions = map(Q_value_curry, possible_actions(current_state))\n",
    "        best_act_idx = Q_value_for_actions.index(max(Q_value_for_actions))\n",
    "        return possible_actions(current_state)[best_act_idx]\n",
    "\n",
    "# return  next state and reward after passed action is applied\n",
    "def transition(state, action):\n",
    "    next_state =  apply_action(state, action)\n",
    "    reward = calc_reward(next_state)\n",
    "    return reward, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do greedy\n",
      "o - -\n",
      "- - -\n",
      "- - -\n",
      "do random\n",
      "o - -\n",
      "- - -\n",
      "x - -\n",
      "do greedy\n",
      "o o -\n",
      "- - -\n",
      "x - -\n",
      "do random\n",
      "o o -\n",
      "- - -\n",
      "x x -\n",
      "do greedy\n",
      "o o o\n",
      "- - -\n",
      "x x -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([((0, 0), 1), ((1, 0), 64), ((1, 64), 2), ((3, 64), 128), ((3, 192), 4)], 1)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization\n",
    "Q_table = [[[0 for a in range(9)] for j in range(2**9)] for i in range(2**9)]\n",
    "Returns = [[[0 for a in range(9)] for j in range(2**9)] for i in range(2**9)]\n",
    "gen_episode(Q_table, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# policy evaluation => policy improvement process\n",
    "def GPIProcess(Q):\n",
    "    episode, reward = gen_episode(Q)\n",
    "    for state, action in episode:\n",
    "        s1, s2, a = state[0], state[1], int(math.log(action,2))\n",
    "        Returns[s1][s2][a].append(reward)\n",
    "        Q[s1][s2][a]= 1.0*sum(Returns[s1][s2][a]) / len(Returns[s1][s2][a])\n",
    "\n",
    "def display_Q_value(Q, state):\n",
    "    Q_value_curry = lambda action: Q[state[0]][state[1]][int(math.log(action,2))]\n",
    "    Q_values =map(Q_value_curry, [1<<n for n in range(9)])\n",
    "    max_len = max([len(str(val)) for val in Q_values])\n",
    "    for i in range(3):\n",
    "        line = []\n",
    "        for j in range(3):\n",
    "            pos = 3*i+j\n",
    "            board = state[0] | state[1]\n",
    "            line.append(str(Q_values[pos]) if (board>>pos)&1 == 0 else \"x\")\n",
    "        print \"%s %s %s\" % tuple([elem.rjust(max_len, '0') for elem in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start learning !! Iterate GPI process for 50000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898170731707 0.869512928443 0.889096573209\n",
      "0.887445887446 0.936357908003 0.813437312537\n",
      "0.961614734726 0.869718309859 0.874560375147\n"
     ]
    }
   ],
   "source": [
    "Q_table = [[[0 for a in range(9)] for j in range(2**9)] for i in range(2**9)]\n",
    "Returns = [[[[] for a in range(9)] for j in range(2**9)] for i in range(2**9)]\n",
    "\n",
    "# Improve agent by iterating GPI process\n",
    "for i in range(50000):\n",
    "    GPIProcess(Q_table)\n",
    "display_Q_value(Q_table, (0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions to play with agent through console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_next_player_agent(state):\n",
    "    first_player_board ,second_player_board = state\n",
    "    count_move = lambda : bin(first_player_board | second_player_board).count(\"1\")\n",
    "    return count_move() % 2 == 0\n",
    "\n",
    "def play_with_agent():\n",
    "    state = (0,0)\n",
    "    while not any(map(is_terminated, state) + [is_draw(state)]):\n",
    "        action, player = None, None\n",
    "        if is_next_player_agent(state):\n",
    "            display_Q_value(Q_table, state)\n",
    "            action = policy(Q_table, state)\n",
    "            player = \"agent\"\n",
    "        else:\n",
    "            action = int(raw_input(\"action => %s\" % possible_actions(state)))\n",
    "            player = \"you\"\n",
    "        state = apply_action(state, action)\n",
    "        visualize_board(state)\n",
    "        print \"player : %s, action : %d\" % (player, int(math.log(action, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play tick-tack-toe with our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898170731707 0.869512928443 0.889096573209\n",
      "0.887445887446 0.936357908003 0.813437312537\n",
      "0.961614734726 0.869718309859 0.874560375147\n",
      "- - -\n",
      "- - -\n",
      "o - -\n",
      "player : agent, action : 6\n",
      "action => [1, 2, 4, 8, 16, 32, 128, 256]16\n",
      "- - -\n",
      "- x -\n",
      "o - -\n",
      "player : you, action : 4\n",
      "0.911076443058 0.817518248175 0.413793103448\n",
      "0.861538461538 0000000000000x 0.661538461538\n",
      "0000000000000x 000000000000.6 00000000000.64\n",
      "o - -\n",
      "- x -\n",
      "o - -\n",
      "player : agent, action : 0\n",
      "action => [2, 4, 8, 32, 128, 256]4\n",
      "o - x\n",
      "- x -\n",
      "o - -\n",
      "player : you, action : 2\n",
      "0000000000000x 0.780487804878 0000000000000x\n",
      "000000000001.0 0000000000000x 0.926829268293\n",
      "0000000000000x 0.918918918919 0.965517241379\n",
      "o - x\n",
      "o x -\n",
      "o - -\n",
      "player : agent, action : 3\n"
     ]
    }
   ],
   "source": [
    "play_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>> Good!! Agent seems understanding how to win!!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898170731707 0.869512928443 0.889096573209\n",
      "0.887445887446 0.936357908003 0.813437312537\n",
      "0.961614734726 0.869718309859 0.874560375147\n",
      "- - -\n",
      "- - -\n",
      "o - -\n",
      "player : agent, action : 6\n",
      "action => [1, 2, 4, 8, 16, 32, 128, 256]1\n",
      "x - -\n",
      "- - -\n",
      "o - -\n",
      "player : you, action : 0\n",
      "0000000000000x 0.872093023256 0.945365853659\n",
      "00000000000.75 0.849056603774 0.789473684211\n",
      "0000000000000x 0.866666666667 0.904761904762\n",
      "x - o\n",
      "- - -\n",
      "o - -\n",
      "player : agent, action : 2\n",
      "action => [2, 8, 16, 32, 128, 256]16\n",
      "x - o\n",
      "- x -\n",
      "o - -\n",
      "player : you, action : 4\n",
      "0000000000000x 000000000000.5 0000000000000x\n",
      "000000000000.0 0000000000000x 0.622641509434\n",
      "0000000000000x 000000000000.0 000000000000.5\n",
      "x - o\n",
      "- x o\n",
      "o - -\n",
      "player : agent, action : 5\n",
      "action => [2, 8, 128, 256]256\n",
      "x - o\n",
      "- x o\n",
      "o - x\n",
      "player : you, action : 8\n"
     ]
    }
   ],
   "source": [
    "play_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>> Woops... Agent made a silly mistake :(`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visited = 321255\n",
      "All state = 2359296\n",
      "visit rate = 0.136166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "visited = reduce(lambda acc, e: sum(e) + acc, np.ndarray.flatten(np.array(Returns)), 0)\n",
    "all_state = reduce(lambda acc, e: acc*e, np.array(Returns).shape, 1)\n",
    "print \"visited = %d\" % visited\n",
    "print \"All state = %d\" % all_state\n",
    "print \"visit rate = %f\" % (1.0 * visited / all_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>> It seems that 50000 GPI iteration was too small (Most of states are never experienced)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
